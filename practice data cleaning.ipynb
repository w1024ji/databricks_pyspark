{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d97c60-5365-41ec-8ddf-b1c3476d6d76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Let's practice data CLEANING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c037e4-e91b-4103-8636-7fc39f7c48b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# let's make some bad data XD\n",
    "bad_data = [\n",
    "    Row(id=1, name=\"Alice\", age=25, score=90.0),\n",
    "    Row(id=2, name=\"Bob\", age=None, score=85.0),   \n",
    "    Row(id=3, name=\"Charlie\", age=35, score=None),\n",
    "    Row(id=4, name=\"David\", age=None, score=None), \n",
    "    Row(id=5, name=\"Eve\", age=40, score=-10.0) \n",
    "]\n",
    "\n",
    "df_bad = spark.createDataFrame(bad_data)\n",
    "display(df_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e59d0c-41fe-460f-9643-91ff7b8046f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# option1: if one field is null -> erase the whole row\n",
    "df_dropped_any = df_bad.na.drop()\n",
    "\n",
    "# option2: delete the column that has null\n",
    "df_dropped_score = df_bad.na.drop(subset=['score'])\n",
    "\n",
    "display(df_dropped_any)\n",
    "display(df_dropped_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29c1c18c-b4c0-461b-a4ca-98b8523c1d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# but if i erase all null, i won't have big data left! \n",
    "# -> so let's fill with something...\n",
    "\n",
    "df_filled_zero = df_bad.na.fill(0)\n",
    "df_filled_custom = df_bad.na.fill({\"age\": 0, \"score\": 50.0})\n",
    "\n",
    "display(df_filled_zero)\n",
    "display(df_filled_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca9c0ee-63c0-4662-a97a-3b120f1a5327",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col, when\n",
    "\n",
    "# hmm.. but filling with 0 or 50 is not a good idea.\n",
    "# how about filling with average of scores?\n",
    "\n",
    "avg_score = df_bad.filter(col(\"score\") > 0).select(avg(\"score\")).first()[0]\n",
    "print(f\"average of scores: {avg_score}\")\n",
    "\n",
    "df_final_cleaned = df_bad.na.fill({\"score\": avg_score})\n",
    "\n",
    "df_final_cleaned = df_final_cleaned.withColumn(\n",
    "    \"score\",\n",
    "    when(col(\"score\") < 0, 0).otherwise(col(\"score\"))\n",
    ")\n",
    "\n",
    "display(df_final_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9561f1f8-1d17-4e87-aca6-c3a02f6313ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Let's join tables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9abbefd8-edd3-4f0a-8052-fbaf76d4764c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# here i made some tables for example\n",
    "users_data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
    "users_df = spark.createDataFrame(users_data, [\"user_id\", \"name\"])\n",
    "\n",
    "orders_data = [(101, 1, \"Laptop\"), (102, 2, \"Mouse\"), (103, 4, \"Keyboard\")]\n",
    "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"user_id\", \"product\"])\n",
    "\n",
    "print(\"--- users table ---\")\n",
    "users_df.show()\n",
    "print(\"--- orders table ---\")\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09508c61-30bb-47d4-952f-ed8f8084d3e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# inner join\n",
    "inner_join_df = users_df.join(orders_df, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "display(inner_join_df) # Charlie disappears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4387ca21-793a-4b95-ac1d-b9382035542b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# left join\n",
    "left_join_df = users_df.join(orders_df, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# na.fill(), when().otherwise -> No Order\n",
    "left_join_cleaned = left_join_df.withColumn(\n",
    "    \"product\",\n",
    "    when(col(\"product\").isNull(), \"No Order\").otherwise(col(\"product\"))\n",
    ")\n",
    "\n",
    "# or, this is also good!\n",
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "left_join_cleaned = left_join_df.withColumn(\n",
    "    \"product\", \n",
    "    coalesce(col(\"product\"), lit(\"No Order\"))\n",
    ")\n",
    "\n",
    "\n",
    "display(left_join_cleaned) # Charlie is alive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56e85599-dfa7-4865-afac-7ed80567cf67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "practice data cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

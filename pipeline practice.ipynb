{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8d63147-7283-4c00-9264-edea0c0cb55c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Let's make semi-auto pipeline\n",
    "#### because i don't know why but i cannot get access to databricks free datasets.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4776a43c-1a00-42f4-8af2-7ce3789f334b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# 1. routes (only made input folder in s3)\n",
    "input_path = \"s3a://wj-spark-bucket/pipeline_input/\"\n",
    "checkpoint_path = \"s3a://wj-spark-bucket/checkpoints/weather_pipeline_v1/\"\n",
    "output_path = \"s3a://wj-spark-bucket/pipeline_output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f39a125e-d11c-4575-99c2-acde72ac634c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. schema! tell auto loader about the schema of the incoming data\n",
    "user_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"score\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e783215c-5f9b-4b9b-b974-e0332f34755e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Auto Loader \n",
    "'''\n",
    "    설계도(user_schema)를 참고해서,\n",
    "    지정된 S3 폴더(input_path)에 새로운 CSV 파일이 들어오는지 감시 카메라(cloudFiles)를 켜고 \n",
    "    실시간으로 읽어라(readStream)\n",
    "'''\n",
    "streaming_df = spark.readStream.format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(user_schema) \\\n",
    "    .load(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d927733-9cd1-42fe-aacf-397326f73c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. filter those score equal or more than 80\n",
    "final_df = streaming_df.filter(col(\"score\") >= 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89d57121-b395-4cf5-8c98-40290ef8f8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. save it \n",
    "'''\n",
    "    가공된 데이터(final_df)를 델타 형식(delta)으로, \n",
    "    기존 데이터 뒤에 덧붙여서(append) 저장해라. \n",
    "    이때 어디까지 했는지 메모(checkpoint)해가면서 하고, \n",
    "    지금 쌓여있는 파일만 처리한 뒤 종료(trigger)해라\n",
    "'''\n",
    "query = final_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9b03e09-e476-4b2c-920f-3faba4e40893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.format(\"delta\").load(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01f0577b-6864-4e1c-a697-05cde08c4635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark's Lazy Evaluation and Execution Plan\n",
    "\n",
    "##### when i ran through the above codes, spark looks like a perfectionist who doesn't start before the plan is all set up. XD\n",
    "\n",
    "\n",
    "###### streaming_df: plan about how it will read data from s3\n",
    "###### final_df(filter): plan about how to filter the data\n",
    "\n",
    "\n",
    "##### Spark starts to execute when it sees writeStream.start(). It then finally starts to read data, filter data ... etc.\n",
    "\n",
    "##### -> so when i call .start(), spark looks at the entire blueprint and realizes: oh. the factory is ready! let's do it!\n",
    "\n",
    "##### that's why if I upload a new csv file to s3 and runs the start(), it does this bluprint - finds the new input, filter, output - and eventually gives the output. \n",
    "\n",
    "--\n",
    "\n",
    "#### My another question was ->\n",
    "##### 'Why can I see the result in full data? Because I uploaded data1.csv, data2.csv and data3.csv seperately, but everytime i run start() and display(), I can see the all users (over 80 score) in one table? How is this possible?'\n",
    "\n",
    "##### I googled it why:\n",
    "###### The files are divided to .parquet files and stored in output folder. And the folder has '_delta_log' which knows what files is for what parquet. \n",
    "###### And when display() runs, spark read '_delta_log' and read them all and show them all on the screen like it is a big table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01d17994-1c0c-4d0a-b8e4-a2f5c52580d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
